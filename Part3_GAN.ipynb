{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Implementation of AE, VAE and GAN\n",
    "This implementation includes the theoritical explamentation and training performance of three different kinds of generative deep learning models, that is, Autoencoder, Variational Autoencoder and Generative Adversarial Network on MNIST dataset. \n",
    "\n",
    "The questions will be covered are:\n",
    "* Autoencoder: model introduction and implementation\n",
    "* Variational Autoencoder: model introduction and implementation, theoritical explamentation\n",
    "* The difference between Autoencoder and Variational Autoencoder\n",
    "* Generative Adversarial Network: model introduction and implementation\n",
    "\n",
    "**Note**: Models are created in Keras\n",
    "\n",
    "## Part 3:  Generative Adversatral Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing:  Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras import backend \n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, BatchNormalization, LeakyReLU, Dropout, Lambda,  UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2DTranspose, Reshape,Activation, LeakyReLU, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import Callback, LearningRateScheduler, ModelCheckpoint\n",
    "from keras.initializers import RandomNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.offsetbox as offsetbox\n",
    "from plot import *\n",
    "import datetime\n",
    "import cv2\n",
    "\n",
    "from IPython.display import display, Image, SVG\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing:  Load the MNIST dataset\n",
    "There are 60,000 MNIST image totally in the train set and 10,000 MNIST image totally in the test set. Each MNIST image is originally with shape (28, 28), each of which is between 0-255 and represents the intensity of a pixel. We normalize the images to the range (0, 1) and reshape them to (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION = 'self_gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'mini'\n",
    "GAN_RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "GAN_RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(GAN_RUN_FOLDER):\n",
    "    os.makedirs(GAN_RUN_FOLDER)\n",
    "    os.makedirs(os.path.join(GAN_RUN_FOLDER, 'viz'))\n",
    "    os.makedirs(os.path.join(GAN_RUN_FOLDER, 'images'))\n",
    "    os.makedirs(os.path.join(GAN_RUN_FOLDER, 'weights'))\n",
    "\n",
    "    \n",
    "input_dim = (28,28,1)\n",
    "discriminator_conv_filters = [64,64,128,128]\n",
    "discriminator_conv_kernel_size = [5,5,5,5]\n",
    "discriminator_conv_strides = [2,2,2,1]\n",
    "\n",
    "discriminator_activation = 'relu'\n",
    "discriminator_dropout_rate = 0.4\n",
    "discriminator_learning_rate = 0.0008\n",
    "n_layers_discriminator = len(discriminator_conv_filters)\n",
    "weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "generator_initial_dense_layer_size = (7, 7, 64)\n",
    "generator_upsample = [2,2, 1, 1]\n",
    "generator_conv_filters = [128,64, 64,1]\n",
    "generator_conv_kernel_size = [5,5,5,5]\n",
    "generator_conv_strides = [1,1, 1, 1]\n",
    "generator_batch_norm_momentum = 0.9\n",
    "\n",
    "generator_activation = 'relu'\n",
    "generator_dropout_rate = None\n",
    "generator_learning_rate = 0.0004\n",
    "n_layers_generator = len(generator_conv_filters)\n",
    "\n",
    "optimiser = 'rmsprop'\n",
    "z_dim = 100\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator\n",
    "def get_activation(activation):\n",
    "    if activation == 'leaky_relu':\n",
    "        layer = LeakyReLU(alpha = 0.2)\n",
    "    else:\n",
    "        layer = Activation(activation)\n",
    "    return layer\n",
    "\n",
    "# define the input of image\n",
    "discriminator_input = Input(shape=input_dim, name='discriminator_input')\n",
    "x = discriminator_input\n",
    "for i in range(n_layers_discriminator):\n",
    "    x = Conv2D(\n",
    "        filters = discriminator_conv_filters[i]\n",
    "        , kernel_size = discriminator_conv_kernel_size[i]\n",
    "        , strides = discriminator_conv_strides[i]\n",
    "        , padding = 'same'\n",
    "        , name = 'discriminator_conv_' + str(i+1)\n",
    "        , kernel_initializer = weight_init\n",
    "        )(x) \n",
    "    # x = BatchNormalization(momentum = discriminator_batch_norm_momentum)(x)\n",
    "    # x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "    x = get_activation(discriminator_activation)(x)\n",
    "x = Flatten()(x)\n",
    "# dense of one unit, with a sigmoid activation function that transforms the output from the dense layer to [0,1]\n",
    "discriminator_output = Dense(1, activation='sigmoid', kernel_initializer = weight_init)(x)\n",
    "# input: image, output: a single number between 0 and 1\n",
    "discriminator = Model(discriminator_input, discriminator_output)\n",
    "\n",
    "# generator: convert a vector in the latent space to an image\n",
    "\n",
    "# unsampling layer: simply repeat each row and column of its input in order to double the size,\n",
    "# then follow this with a normal convolutoinal layer with stride 1 to perform the convolution operation. \n",
    "# instead of filling the gaps between pixels with zeros, unsampling just repeats the existing pixel values. \n",
    "# Conv2DTranspose method can lead to artifacts, or small checkerboard patterns in the output image that spoil the quality of the ouput\n",
    "\n",
    "generator_input = Input(shape=(z_dim,), name='generator_input')\n",
    "x = generator_input\n",
    "x = Dense(np.prod(generator_initial_dense_layer_size), kernel_initializer = weight_init)(x)\n",
    "# x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "\n",
    "x = get_activation(generator_activation)(x)\n",
    "x = Reshape(generator_initial_dense_layer_size)(x)  #reshape to [7,7,64]\n",
    "# x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "\n",
    "for i in range(n_layers_generator):\n",
    "    if generator_upsample[i] == 2:\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(\n",
    "            filters = generator_conv_filters[i]\n",
    "            , kernel_size = generator_conv_kernel_size[i]\n",
    "            , padding = 'same'\n",
    "            , name = 'generator_conv_' + str(i+1)\n",
    "            , kernel_initializer = weight_init\n",
    "        )(x)\n",
    "    else:\n",
    "        x = Conv2DTranspose(\n",
    "            filters = generator_conv_filters[i]\n",
    "            , kernel_size = generator_conv_kernel_size[i]\n",
    "            , padding = 'same'\n",
    "            , strides = generator_conv_strides[i]\n",
    "            , name = 'generator_conv_' + str(i+1)\n",
    "            , kernel_initializer = weight_init\n",
    "            )(x)\n",
    "\n",
    "    if i < n_layers_generator - 1:\n",
    "        # x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "        x = get_activation(generator_activation)(x)\n",
    "    else:\n",
    "        # tansform the output to the range [-1, 1] to match the original image domain\n",
    "        x = Activation('tanh')(x)\n",
    "generator_output = x\n",
    "# accept a vector of lenght 100 and output a tensor of shape [28, 28, 1]\n",
    "generator = Model(generator_input, generator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process: \n",
    "1. some of the images are randomly selected real observations from the training set and some are outputs from the generator; \n",
    "2. train the discriminator to learn how to tell the difference between the original and generated images, outputting value near fot the true images and values near 0 for the fake images. \n",
    "3. to train the generator, we connect it to the discriminator to create a Keras model. Feed the output from the generator into the discriminator so that the output from this combined model is the probability that the generated image is real, according to the discriminator. \n",
    "\n",
    "The loss function is the binary cross-entropy loss between the output from the discriminator and the response vector of 1. \n",
    "\n",
    "Freese the weights of the discriminator while we are training the combined model, so that only the generator's weights are updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(m, val):\n",
    "    m.trainable = val\n",
    "    for l in m.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "# compiled the discriminator with binary cross-entropy loss, as the response is binary and we have one output unit with sigmoid avtivation.\n",
    "discriminator.compile(optimizer= RMSprop(lr=discriminator_learning_rate), loss = 'binary_crossentropy', \n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "# freeze the discriminator weights\n",
    "set_trainable(discriminator, False)\n",
    "\n",
    "# input is a 100 dimensional latent vector, passed through the generator and frozen discriminator to produce the output prob\n",
    "model_input = Input(shape=(z_dim,), name='model_input')\n",
    "model_output = discriminator(generator(model_input))\n",
    "GAN_model = Model(model_input, model_output)\n",
    "\n",
    "# using a binary cross-entropy loss for the combined model\n",
    "GAN_model.compile(optimizer=  RMSprop(lr=generator_learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "set_trainable(discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GAN by alternating training of the discriminator and generator \n",
    "def train_discriminator(x_train, batch_size):\n",
    "\n",
    "    valid = np.ones((batch_size,1))\n",
    "    fake = np.zeros((batch_size,1))\n",
    "\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    true_imgs = x_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    # one batch update of the discriminator involves first training on a batch of true images with the response 1\n",
    "    d_loss_real, d_acc_real =   discriminator.train_on_batch(true_imgs, valid)\n",
    "    # on a batch of generated image with the response 0\n",
    "    d_loss_fake, d_acc_fake =   discriminator.train_on_batch(gen_imgs, fake)\n",
    "    d_loss =  0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "    return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "\n",
    "def train_generator(batch_size):\n",
    "    \n",
    "    # on a batch of generated images with the response 1\n",
    "    # the generator weights will move in the direction that allows it to better generate images to fool the discriminator\n",
    "    valid = np.ones((batch_size,1))\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    return GAN_model.train_on_batch(noise, valid)\n",
    "\n",
    "\n",
    "def sample_images( run_folder):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % epoch))\n",
    "    plt.close()\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "print_every_n_batches = 500\n",
    "\n",
    "epochs = 2\n",
    "iterations = int(x_train.shape[0] / BATCH_SIZE)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        d = train_discriminator(x_train, BATCH_SIZE)\n",
    "        g = train_generator(BATCH_SIZE)\n",
    "\n",
    "        d_losses.append(d)\n",
    "        g_losses.append(g)\n",
    "\n",
    "        if iteration % print_every_n_batches == 0:\n",
    "            print (\"epoch %d iteration %d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, iteration, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "            sample_images(GAN_RUN_FOLDER)\n",
    "            GAN_model.save_weights(os.path.join(GAN_RUN_FOLDER, 'weights/weights-%d-%d.h5' % (epoch, iteration)))\n",
    "            GAN_model.save_weights(os.path.join(GAN_RUN_FOLDER, 'weights/weights.h5'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[0] for x in d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x[1] for x in d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[2] for x in d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[0] for x in g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(0, 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x[3] for x in d_losses], color='black', linewidth=0.25)\n",
    "plt.plot([x[4] for x in d_losses], color='green', linewidth=0.25)\n",
    "plt.plot([x[5] for x in d_losses], color='red', linewidth=0.25)\n",
    "plt.plot([x[1] for x in g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.xlabel('batch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "plt.xlim(0, 2000)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpython",
   "language": "python",
   "name": "tfpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
